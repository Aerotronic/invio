void exponential_map(const cv::Mat &v, cv::Mat dt, cv::Mat dR)
{
  double vx = v.at<double>(0,0);
  double vy = v.at<double>(1,0);
  double vz = v.at<double>(2,0);
  double vtux = v.at<double>(3,0);
  double vtuy = v.at<double>(4,0);
  double vtuz = v.at<double>(5,0);
  cv::Mat tu = (cv::Mat_<double>(3,1) << vtux, vtuy, vtuz); // theta u
  cv::Rodrigues(tu, dR);

  double theta = sqrt(tu.dot(tu));
  double sinc = (fabs(theta) < 1.0e-8) ? 1.0 : sin(theta) / theta;
  double mcosc = (fabs(theta) < 2.5e-4) ? 0.5 : (1.-cos(theta)) / theta / theta;
  double msinc = (fabs(theta) < 2.5e-4) ? (1./6.) : (1.-sin(theta)/theta) / theta / theta;

  dt.at<double>(0,0) = vx*(sinc + vtux*vtux*msinc)
        + vy*(vtux*vtuy*msinc - vtuz*mcosc)
        + vz*(vtux*vtuz*msinc + vtuy*mcosc);

  dt.at<double>(1,0) = vx*(vtux*vtuy*msinc + vtuz*mcosc)
        + vy*(sinc + vtuy*vtuy*msinc)
        + vz*(vtuy*vtuz*msinc - vtux*mcosc);

  dt.at<double>(2,0) = vx*(vtux*vtuz*msinc - vtuy*mcosc)
        + vy*(vtuy*vtuz*msinc + vtux*mcosc)
        + vz*(sinc + vtuz*vtuz*msinc);
}

//! [Estimation function]
void VIO::pose_gauss_newton(const std::vector< cv::Point3d > &wX,
                       const std::vector< cv::Point2d > &x,
                       cv::Mat &ctw, cv::Mat &cRw)
//! [Estimation function]
{
  //! [Gauss-Newton]
  int npoints = (int)wX.size();
  cv::Mat J(2*npoints, 6, CV_64F);
  cv::Mat cX;
  double lambda = 0.25;
  cv::Mat err, sd(2*npoints, 1, CV_64F), s(2*npoints, 1, CV_64F);
  cv::Mat xq(npoints*2, 1, CV_64F);
  // From input vector x = (x, y) we create a column vector xn = (x, y)^T to ease computation of e_q
  cv::Mat xn(npoints*2, 1, CV_64F);
  //vpHomogeneousMatrix cTw_ = cTw;
  double residual=0, residual_prev;
  cv::Mat Jp;

  // From input vector x = (x, y, 1)^T we create a new one xn = (x, y)^T to ease computation of e_q
  for (int i = 0; i < x.size(); i ++) {
    xn.at<double>(i*2,0)   = x[i].x; // x
    xn.at<double>(i*2+1,0) = x[i].y; // y
  }

  int iteration = 0;
  // Iterative Gauss-Newton minimization loop
  do {
	iteration++;

    for (int i = 0; i < npoints; i++) {
      cX = cRw * cv::Mat(wX[i]) + ctw;                      // Update cX, cY, cZ
      // Update x(q)
      xq.at<double>(i*2,0)   = cX.at<double>(0,0) / cX.at<double>(2,0); // x(q) = cX/cZ
      xq.at<double>(i*2+1,0) = cX.at<double>(1,0) / cX.at<double>(2,0); // y(q) = cY/cZ

      // Update J using equation (11)
      J.at<double>(i*2,0) = -1/cX.at<double>(2,0);          // -1/cZ
      J.at<double>(i*2,1) = 0;
      J.at<double>(i*2,2) = x[i].x / cX.at<double>(2,0);    // x/cZ
      J.at<double>(i*2,3) = x[i].x * x[i].y;                // xy
      J.at<double>(i*2,4) = -(1 + x[i].x * x[i].x);         // -(1+x^2)
      J.at<double>(i*2,5) = x[i].y;                         // y

      J.at<double>(i*2+1,0) = 0;
      J.at<double>(i*2+1,1) = -1/cX.at<double>(2,0);        // -1/cZ
      J.at<double>(i*2+1,2) = x[i].y / cX.at<double>(2,0);  // y/cZ
      J.at<double>(i*2+1,3) = 1 + x[i].y * x[i].y;          // 1+y^2
      J.at<double>(i*2+1,4) = -x[i].x * x[i].y;             // -xy
      J.at<double>(i*2+1,5) = -x[i].y;                      // -x
    }

    cv::Mat e_q = xq - xn;                                  // Equation (7)

    cv::Mat Jp = J.inv(cv::DECOMP_SVD);                     // Compute pseudo inverse of the Jacobian
    cv::Mat dq = -lambda * Jp * e_q;                        // Equation (10)

    cv::Mat dctw(3,1,CV_64F), dcRw(3,3,CV_64F);
    exponential_map(dq, dctw, dcRw);

    cRw = dcRw.t() * cRw;                                   // Update the pose
    ctw = dcRw.t() * (ctw - dctw);

    residual_prev = residual;                               // Memorize previous residual
    residual = e_q.dot(e_q);                                // Compute the actual residual

  } while (iteration <= MAX_GN_ITERS && fabs(residual - residual_prev) > 0);
  //! [Gauss-Newton]
}

/*
 * structure only bundle adjustment between two frames
 */
void VIO::motionOnlyBundleAdjustment(Frame& cf) {
	g2o::SparseOptimizer optimizer; // this is the g2o optimizer which ultimately solves the problem
	optimizer.setVerbose(true); // set the verbosity of the optimizer

	g2o::BlockSolver_6_3::LinearSolverType * linearSolver; //create a linear solver type pointer
	linearSolver = new g2o::LinearSolverCholmod<g2o::BlockSolver_6_3::PoseMatrixType>(); // use a cholesky linear solver

	g2o::BlockSolver_6_3 * solver_ptr = new g2o::BlockSolver_6_3(linearSolver); // finally create the solver

	g2o::OptimizationAlgorithmLevenberg* solver =
			new g2o::OptimizationAlgorithmLevenberg(solver_ptr); // create a LM optimization type using the cholmod solver

	solver->setMaxTrialsAfterFailure(5);
	optimizer.setAlgorithm(solver); // add the LM to the optimizer

	//configure the camera parameters
	g2o::CameraParameters * cam_params = new g2o::CameraParameters(1.0, Eigen::Vector2d(0.0, 0.0), 0.0);
	cam_params->setId(0);
	if(!optimizer.addParameter(cam_params)){
		ROS_FATAL("Could not add the camera parameters to g2o");
	}

	// this should contain the rotation and translation from the base of the system to the camera
	tf::StampedTransform b2c;
	try {
		this->ekf.tf_listener.lookupTransform(this->CoM_frame, this->camera_frame,
				ros::Time(0), b2c);
	} catch (tf::TransformException& e) {
		ROS_WARN_STREAM(e.what());
	}

	VIOState x_currentfFrame = transformState(cf.state, b2c);

	//VIOState x_keyFrame = transformState(kf.frame->state, b2c);

	//ROS_DEBUG_STREAM_ONCE("dx of b2c: " << b2c.getOrigin().getX());
	//double baseline_val = (cf.state.getr() - kf.frame->state.getr()).norm(); // this is the estimated distance between the two frames
	//g2o::VertexSCam::setKcam(1.0, 1.0, 0.0, 0.0, baseline_val); // set up the camera parameters

	// setup vertex 1 aka keyFrame1
	//const int KEYFRAME_VERTEX_ID = 0;

	//g2o::VertexSE3Expmap * kf_vertex = new g2o::VertexSE3Expmap(); // create a new vertex for this frame;

	//kf_vertex->setId(KEYFRAME_VERTEX_ID); // the id of the first keyframe is 0

	//kf_vertex->setEstimate(g2o::SE3Quat(x_keyFrame.getQuaternion(), x_keyFrame.getr())); // set the estimate of this frame
	//kf_vertex->setFixed(true); // the keyframe's position is fixed

	//optimizer.addVertex(kf_vertex); // add this vertex to the optimizer

	//setup vertex 2 aka current Frame
	const int CURRENTFRAME_VERTEX_ID = 0;

	g2o::VertexSE3Expmap * cf_vertex = new g2o::VertexSE3Expmap(); // create a new vertex for this frame;

	cf_vertex->setId(CURRENTFRAME_VERTEX_ID); // the id of the first current frame is 1

	cf_vertex->setEstimate(g2o::SE3Quat(x_currentfFrame.getQuaternion(), x_currentfFrame.getr())); // set the estimate of this frame
	// if this is structure only the vertex is fixed
	// otherwise it is not fixed and can be optimized
	cf_vertex->setFixed(false); // the current frame's position is not fixed

	optimizer.addVertex(cf_vertex); // add the vertex to the problem

	// now the camera vertices are part of the problem
	const int POINTS_STARTING_ID = 1;

	int point_id = POINTS_STARTING_ID; // start the next vertex ids from 2
	int number_of_points = 0;

	ROS_DEBUG("camera vertices have been setup");
	// setup the rest of the graph optimization problem
	//TODO setup points

	for(auto& ft : currentFrame().features)
	{

	}


	ROS_DEBUG("preparing to initilize g2o");
	bool initStatus = optimizer.initializeOptimization(); // set up the problem for optimization
	ROS_WARN_STREAM_COND(!initStatus, "something went wrong when initializing the bundle adjustment problem");
	ROS_DEBUG("initilized g2o");

	//optimizer.setVerbose(true);


}





/*
 * NOTE: the position of the state, the last state, and the keyframe state must be estimated by this point
 *
 * this function cycles through all matched features from key frame 0,
 * transforms their depth into the current frame
 * calculates their depth in the current frame
 * updates their depth
 * if their depth variance is good enough
 * 	convert the point to a 3d features for future motion estimation
 *
 * x is the state of the current frame
 */
void VIO::updateFeatureDepths(VIOState x)
{
	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException& e){
		ROS_WARN_STREAM(e.what());
	}

	Frame& cf = currentFrame();
	Frame& lf = lastFrame();
	KeyFrameInfo& kf = this->keyFrames.at(0);
	ROS_ASSERT(kf.nextFeatureID = this->frameBuffer.at(kf.frameBufferIndex).nextFeatureID);

	cv::Matx34d P1(1, 0, 0, 0,
			0, 1, 0, 0,
			0, 0, 1, 0);

	tf::Transform tf_current = this->cameraTransformFromState(x, base2cam);

	// tf_last * P1_last = tf_current => tf_last.inv() * tf_current = P1_last
	tf::Transform last2current = this->cameraTransformFromState(lf.state, base2cam).inverse() * tf_current;

	// tf_current * P2 = tf_kf => tf_kf * tf_current.inv = P2
	//tf::Transform P2_temp = tf_current.inverse() * this->cameraTransformFromState(this->frameBuffer.at(kf.frameBufferIndex).state, base2cam);
	//cv::Matx34d P2 = tfTransform2RtMatrix(P2_temp); // this is the transform which converts points in the currentframe to points in the keyframe

	//go through each current feature and transform its depth from the last frame
	for(auto& e : cf.features)
	{
		VIOFeature2D& last_ft = (e.isMatched()) ? lf.features.at(e.getMatchedIndex()) : e;
		//last_ft = lf.features.at(e.getMatchedIndex()); // get the last feature which matches this one
		if(!e.isMatched())
			continue;

		ROS_ASSERT(last_ft.getFeatureID() == e.getMatchedID());

		tf::Vector3 transformedPoint = last2current * (last_ft.getFeatureDepth() * tf::Vector3(last_ft.getUndistorted().x, last_ft.getUndistorted().y, 1.0)); // transform the 3d point from the last frame into the current frame

		//extract the depth from the transformed point and set it
		e.setFeatureDepth(transformedPoint.z()); // the depth would be equal to the new z from the transformed point
		//ROS_DEBUG_STREAM(e.getFeatureDepth());
	}
}



cv::Mat VIO::reproject3dPoints(cv::Mat img_in, VIOState x)
{
	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException& e){
		ROS_WARN_STREAM(e.what());
	}

	tf::Transform cam2world = (tf::Transform(x.getTFQuaternion(), tf::Vector3(x.x(), x.y(), x.z())) * base2cam).inverse();

	tf::Quaternion tf_q = cam2world.getRotation();
	cv::Mat temp = img_in;

	Eigen::Quaternionf q;
	q.w() = tf_q.w();
	q.x() = tf_q.x();
	q.y() = tf_q.y();
	q.z() = tf_q.z();

	cv::Matx33f tK = currentFrame().K;

	cv::Matx33f R;
	cv::eigen2cv(q.matrix(), R);

	cv::Matx31f t;
	t(0) = cam2world.getOrigin().getX();
	t(1) = cam2world.getOrigin().getY();
	t(2) = cam2world.getOrigin().getZ();

	cv::Matx34f P;
	cv::hconcat(R, t, P);

	//ROS_DEBUG_STREAM(active3DFeatures.begin() << " and " << active3DFeatures.end());

	for(int i = 0; i < active3DFeatures.size(); i++)
	{
		cv::Matx41f X;
		X(0) = active3DFeatures.at(i).position(0);
		X(1) = active3DFeatures.at(i).position(1);
		X(2) = active3DFeatures.at(i).position(2);
		X(3) = 1.0;

		cv::Matx31f u = tK * P * X;

		cv::drawMarker(temp, cv::Point2f(u(0) / u(2), u(1) / u(2)), cv::Scalar(0, 255, 255), cv::MARKER_CROSS, 6, 1);
		ROS_DEBUG_STREAM("reproj Pt: " << u(0)/u(2) << ", " << u(1)/u(2));
	}

	return temp;
}




void VIO::drawKeyFrames()
{
	cv::Mat img1, img2, img3, img4, img5;
	img1 = currentFrame().image;
	img2 = frameBuffer.at(keyFrames.at(0).frameBufferIndex).image;


	cv::cvtColor(img1, img1, CV_GRAY2BGR);
	cv::cvtColor(img2, img2, CV_GRAY2BGR);
	cv::cvtColor(img3, img3, CV_GRAY2BGR);
	cv::cvtColor(img4, img4, CV_GRAY2BGR);
	cv::cvtColor(img5, img5, CV_GRAY2BGR);

	cv::Matx33f F;
	cv::Matx33f tK;

	//do work on images
	Frame cf = currentFrame();
	cf.K.copyTo(tK);

	for(int i = 0; i < cf.features.size(); i++)
	{
		cv::Matx31f u;
		u(0) = cf.features.at(i).getFeaturePosition().x;
		u(1) = cf.features.at(i).getFeaturePosition().y;

		cv::Scalar color = HSVtoBGR((cf.features.at(i).getFeatureDepth() / 5.0) * 255.0, 1.0, 1.0);

		cv::drawMarker(img1, cv::Point2f(u(0), u(1)), color, cv::MARKER_DIAMOND, 6);
	}
	F = keyFrames.at(0).F;
	//ROS_DEBUG_STREAM("feat: " <<  keyFrames.at(0).matchedFeatures.size());
	for(auto e : keyFrames.at(0).matchedFeatures)
	{
		cv::Matx31f u;
		u(0) = e.getFeaturePosition().x;
		u(1) = e.getFeaturePosition().y;

		cv::drawMarker(img2, cv::Point2f(u(0), u(1)), cv::Scalar(0, 255, 255), cv::MARKER_DIAMOND, 12);

		img2 = drawEpiLines(F, e.getUndistorted(), tK, img2);
	}
	//F = keyFrames.at(1).F;
	for(auto e : keyFrames.at(1).matchedFeatures)
	{
		cv::Matx31f u;
		u(0) = e.getFeaturePosition().x;
		u(1) = e.getFeaturePosition().y;

		cv::drawMarker(img3, cv::Point2f(u(0), u(1)), cv::Scalar(0, 255, 255), cv::MARKER_DIAMOND, 12);

		//img3 = drawEpiLines(F, e.getUndistorted(), tK, img3);
	}
	//F = keyFrames.at(2).F;
	for(auto e : keyFrames.at(2).matchedFeatures)
	{
		cv::Matx31f u;
		u(0) = e.getFeaturePosition().x;
		u(1) = e.getFeaturePosition().y;

		cv::drawMarker(img4, cv::Point2f(u(0), u(1)), cv::Scalar(0, 255, 255), cv::MARKER_DIAMOND, 12);

		//img4 = drawEpiLines(F, e.getUndistorted(), tK, img4);
	}

	//F = tK.t() * keyFrames.at(3).F * tK; // convert the essential mat into the fundamental mat
	//F = keyFrames.at(3).F;

	for(auto e : keyFrames.at(3).matchedFeatures)
	{
		cv::Matx31f u;
		u(0) = e.getFeaturePosition().x;
		u(1) = e.getFeaturePosition().y;

		cv::drawMarker(img5, cv::Point2f(u(0), u(1)), cv::Scalar(0, 255, 255), cv::MARKER_DIAMOND, 12);

		//img5 = drawEpiLines(F, e.getUndistorted(), tK, img5);

	}



	cv::Mat img2_s, img3_s, img4_s, img5_s;
	cv::resize(img2, img2_s, cv::Size(320, 256));
	cv::resize(img3, img3_s, cv::Size(320, 256));
	cv::resize(img4, img4_s, cv::Size(320, 256));
	cv::resize(img5, img5_s, cv::Size(320, 256));


	cv::Mat final = cv::Mat(cv::Size(640, 1024), CV_8UC3);
	//cv::Mat roi1 = cv::Mat(final, cv::Rect(0, 0, 640, 512));
	img1.copyTo(final(cv::Rect(0, 0, 640, 512)));
	//cv::Mat roi2 = cv::Mat(final, cv::Rect(0, 512, 320, 256));
	img2_s.copyTo(final(cv::Rect(0, 512, 320, 256)));
	//cv::Mat roi3 = cv::Mat(final, cv::Rect(320, 512, 320, 256));
	img3_s.copyTo(final(cv::Rect(320, 512, 320, 256)));
	//cv::Mat roi4 = cv::Mat(final, cv::Rect(0, 768, 320, 256));
	img4_s.copyTo(final(cv::Rect(0, 768, 320, 256)));
	//cv::Mat roi5 = cv::Mat(final, cv::Rect(320, 768, 320, 256));
	img5_s.copyTo(final(cv::Rect(320, 768, 320, 256)));


	//cv::namedWindow("debug", cv::WINDOW_AUTOSIZE);
	cv::imshow("debug", final);
	cv::waitKey(1);
}

double VIO::computeKeyFramePixelDelta(Frame cf, KeyFrameInfo& keyFrame)
{
	Frame matchFrame = this->frameBuffer.at(keyFrame.frameBufferIndex);
	ROS_ASSERT(matchFrame.nextFeatureID == keyFrame.nextFeatureID);

	std::vector<VIOFeature2D> matchedFeatures;

	double deltaSum = 0;

	for(const int& index : keyFrame.currentFrameIndexes)
	{
		if(keyFrame.frameBufferIndex == 0)
		{
			deltaSum = 0;
			break;
		}

		VIOFeature2D& ft2 = cf.features.at(index);

		int matchFeatureIndex = ft2.getMatchedIndexDeque().at(keyFrame.frameBufferIndex - 1);
		int matchFeatureID = ft2.getMatchedIDDeque().at(keyFrame.frameBufferIndex - 1);

		VIOFeature2D& ft1 = matchFrame.features.at(matchFeatureIndex);
		ROS_ASSERT(ft1.getFeatureID() == matchFeatureID);

		matchedFeatures.push_back(ft1);

		deltaSum += this->manhattan(ft1.getUndistorted(true), ft2.getUndistorted(true));
	}

	keyFrame.matchedFeatures = matchedFeatures;

	return deltaSum / (double)keyFrame.currentFrameIndexes.size();
}


VIOFeature2D VIO::getCorrespondingFeature(VIOFeature2D currFeature, Frame lastFrame)
{
	//ROS_ASSERT(currFeature.isMatched());

	VIOFeature2D lastFeature = lastFrame.features.at(currFeature.getMatchedIndex());

	ROS_ASSERT(lastFeature.getFeatureID() == currFeature.getMatchedID());

	return lastFeature;
}

double VIO::computeFundamentalMatrix(cv::Mat& F, KeyFrameInfo& kf)
{
	std::vector<cv::Point2f> pt1_temp, pt2_temp;
	cv::Mat mask;
	this->computeFundamentalMatrix(F, kf, pt1_temp, pt2_temp, mask);
}

double VIO::computeFundamentalMatrix(cv::Mat& F, KeyFrameInfo& kf, std::vector<cv::Point2f>& pt1_temp, std::vector<cv::Point2f>& pt2_temp, cv::Mat& mask)
{
	Frame& cf = currentFrame();
		Frame& mf = this->frameBuffer.at(kf.frameBufferIndex);
		ROS_ASSERT(mf.nextFeatureID == kf.nextFeatureID); // ensure that this kf is matched to the correct frame

		//std::vector<cv::Point2f> pt1_temp, pt2_temp;

		//push back undistorted and normalized image coordinates into their respective vecs
		for(auto e : kf.matchedFeatures)
		{
			pt1_temp.push_back(e.getUndistorted());
		}

		for(auto e : kf.currentFrameIndexes)
		{
			pt2_temp.push_back(cf.features.at(e).getUndistorted());
		}

		//cv::Mat mask; // this is the mask which shows what features were used for motion estimation
		cv::Mat E = cv::findEssentialMat(pt1_temp, pt2_temp, cv::Mat::eye(cv::Size(3, 3), CV_32F), cv::RANSAC, 0.999, 0.1, mask); // compute the essential/fundamental matrix

		//compute the error in the motion estimate
		double essential_error = 0;
		for(int i = 0; i < pt1_temp.size(); i++)
		{
			cv::Matx31f u1, u2;

			u1(0) = pt1_temp.at(i).x;
			u1(1) = pt1_temp.at(i).y;
			u1(2) = 1.0;

			u2(0) = pt2_temp.at(i).x;
			u2(1) = pt2_temp.at(i).y;
			u2(2) = 1.0;

			Eigen::Matrix<float, 3, 1> u1_eig, u2_eig;
			Eigen::Matrix<float, 3, 3> E_eig;

			cv::cv2eigen(u1, u1_eig);
			cv::cv2eigen(u2, u2_eig);
			cv::cv2eigen(E, E_eig);

			essential_error += abs((u2_eig.transpose() * E_eig * u1_eig)(0, 0));
		}

		//pt1 = pt1_temp;
		//pt2 = pt2_temp;

		F = E;
		return essential_error / pt1_temp.size();
}

/*
 * uses the features of the current frame and the keyframe to estimate the scaled motion between the two frames and returns a current pose estimate
 * along with the certianty of the estimate
 */
double VIO::computeFundamentalMatrix(cv::Mat& F, cv::Matx33d& R, cv::Matx31d& t, KeyFrameInfo& kf)
{
	std::vector<cv::Point2f> pt1, pt2;
	cv::Mat mask;

	double essential_error = this->computeFundamentalMatrix(F, kf, pt1, pt2, mask);

	cv::Mat R_temp, t_temp;

	cv::recoverPose(F, pt1, pt2, cv::Mat::eye(cv::Size(3, 3), CV_32F), R_temp, t_temp, mask); // chooses one of the four possible solutions for the motion using state estimates

	R_temp.convertTo(R_temp,  R.type);
	R_temp.copyTo(R);
	t_temp.convertTo(t_temp,  t.type);
	t_temp.copyTo(t);

	return essential_error;

}


void VIO::bruteForceKeyFrameUpdate()
{
	ros::Time start = ros::Time::now();

	Frame cf = this->frameBuffer.at(0);

	int keyFramesSet = 0;

	//initialize lastIndexes with all indexes from the current frame
	std::vector<int> lastIndexes;
	for(int i = 0; i < cf.features.size(); i++)
		lastIndexes.push_back(i);

	int kfLvl1 = round(cf.features.size() * KEYFRAME_LEVEL_1);
	int kfLvl2 = round(cf.features.size() * KEYFRAME_LEVEL_2);
	int kfLvl3 = round(cf.features.size() * KEYFRAME_LEVEL_3);
	int kfLvl4 = round(cf.features.size() * KEYFRAME_LEVEL_4);

	//ROS_DEBUG_STREAM(lastIndexes.size());

	for(int i = 1; i < this->frameBuffer.size();)
	{
		if(keyFramesSet == NUM_KEYFRAMES)
			break;

		std::vector<int> tempIndexes;
		tempIndexes.reserve(lastIndexes.size());

		// these are for collecting the smallest index
		int smallestDequeIndex = 0;
		int smallestDequeSize;
		if(lastIndexes.size() > 0){
			smallestDequeSize = cf.features.at(lastIndexes.at(0)).getMatchedIndexDeque().size();
		}
		else
		{
			break;
		}

		for(const int& cfIndex : lastIndexes)
		{

			int thisDequeSize = cf.features.at(cfIndex).getMatchedIndexDeque().size();
			if(thisDequeSize >= i)
			{
				if(cf.features.at(cfIndex).getMatchedIndexDeque().size() < smallestDequeSize)
				{
					smallestDequeSize = cf.features.at(cfIndex).getMatchedIndexDeque().size();
					smallestDequeIndex = cfIndex;
				}

				tempIndexes.push_back(cfIndex);
			}

		}

		int tempIndexSize = tempIndexes.size();

		//ROS_DEBUG_STREAM("features left: " << tempIndexSize);

		if(keyFramesSet < 1 && (tempIndexSize < kfLvl1 || tempIndexSize < 8))
		{
			keyFramesSet++;
			keyFrames.at(0).currentFrameIndexes = lastIndexes;
			keyFrames.at(0).frameBufferIndex = i - 1;
			keyFrames.at(0).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
			keyFrames.at(0).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(0));

			//ROS_DEBUG_STREAM("setting keyframe 1 from inside with index " << keyFrames.at(0).frameBufferIndex);
		}

		if(keyFramesSet < 2 && (tempIndexSize < kfLvl2 || tempIndexSize < 8))
		{
			keyFramesSet++;
			keyFrames.at(1).currentFrameIndexes = lastIndexes;
			keyFrames.at(1).frameBufferIndex = i - 1;
			keyFrames.at(1).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
			keyFrames.at(1).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(1));

			//ROS_DEBUG_STREAM("setting keyframe 2 from inside with index " << keyFrames.at(1).frameBufferIndex);
		}

		if(keyFramesSet < 3 && (tempIndexSize < kfLvl3 || tempIndexSize < 8))
		{
			keyFramesSet++;
			keyFrames.at(2).currentFrameIndexes = lastIndexes;
			keyFrames.at(2).frameBufferIndex = i - 1;
			keyFrames.at(2).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
			keyFrames.at(2).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(2));

			//ROS_DEBUG_STREAM("setting keyframe 3 from inside with index " << keyFrames.at(2).frameBufferIndex);
		}

		if(keyFramesSet < 4 && (tempIndexSize < kfLvl4 || tempIndexSize < 8))
		{
			keyFramesSet++;
			keyFrames.at(3).currentFrameIndexes = lastIndexes;
			keyFrames.at(3).frameBufferIndex = i - 1;
			keyFrames.at(3).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
			keyFrames.at(3).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(3));

			//ROS_DEBUG_STREAM("setting keyframe 4 from inside with index " << keyFrames.at(3).frameBufferIndex);
		}

		//ROS_DEBUG_STREAM("current index: " << i);
		//ROS_DEBUG_STREAM("smallest deque size: " << smallestDequeSize);
		//ROS_DEBUG_STREAM("optimization wants to skip to " << smallestDequeSize + 1 << " and get last features from " << smallestDequeSize);

		// now skip forward to the point just before a feature is lost
		if(smallestDequeSize < this->frameBuffer.size())
		{

			i = smallestDequeSize; // will be iterated once

			tempIndexes.clear(); // empty the temp indexes to be filled again

			for(const int& cfIndex : lastIndexes)
			{

				int thisDequeSize = cf.features.at(cfIndex).getMatchedIndexDeque().size();
				if(thisDequeSize >= smallestDequeSize)
				{
					tempIndexes.push_back(cfIndex);
				}
			}
		}
		else
		{
			lastIndexes = tempIndexes;
			break;
		}

		lastIndexes = tempIndexes;
		i++;
	}

	// in the case that a key frame is not set in the for loop above
	// set each keyframe to be the last frame

	int i = this->frameBuffer.size(); // assuming that I use i - 1 (which i do, this is pure laziness)
	if(keyFramesSet <= 3)
	{
		keyFrames.at(3).currentFrameIndexes = lastIndexes;
		keyFrames.at(3).frameBufferIndex = i - 1;
		keyFrames.at(3).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
		keyFrames.at(3).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(3));

		//ROS_DEBUG_STREAM("setting keyframe 4 from outside with index " << keyFrames.at(3).frameBufferIndex);
	}

	if(keyFramesSet <= 2)
	{
		keyFrames.at(2).currentFrameIndexes = lastIndexes;
		keyFrames.at(2).frameBufferIndex = i - 1;
		keyFrames.at(2).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
		keyFrames.at(2).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(2));

		//ROS_DEBUG_STREAM("setting keyframe 3 from outside with index " << keyFrames.at(2).frameBufferIndex);
	}

	if(keyFramesSet <= 1)
	{
		keyFrames.at(1).currentFrameIndexes = lastIndexes;
		keyFrames.at(1).frameBufferIndex = i - 1;
		keyFrames.at(1).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
		keyFrames.at(1).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(1));

		//ROS_DEBUG_STREAM("setting keyframe 2 from outside with index " << keyFrames.at(1).frameBufferIndex);
	}
	if(keyFramesSet == 0)
	{
		keyFrames.at(0).currentFrameIndexes = lastIndexes;
		keyFrames.at(0).frameBufferIndex = i - 1;
		keyFrames.at(0).nextFeatureID = this->frameBuffer.at(i - 1).nextFeatureID;
		keyFrames.at(0).pixelDelta = this->computeKeyFramePixelDelta(cf, keyFrames.at(0));

		//ROS_DEBUG_STREAM("setting keyframe 1 from outside with index " << keyFrames.at(0).frameBufferIndex);
	}

	//DEBUG
	//ROS_DEBUG_STREAM("KeyFrame 1 index: " << keyFrames.at(0).frameBufferIndex << " features: " << keyFrames.at(0).currentFrameIndexes.size() << " pxlDelta: " << keyFrames.at(0).pixelDelta);
	//ROS_DEBUG_STREAM("KeyFrame 2 index: " << keyFrames.at(1).frameBufferIndex << " features: " << keyFrames.at(1).currentFrameIndexes.size() << " pxlDelta: " << keyFrames.at(1).pixelDelta);
	//ROS_DEBUG_STREAM("KeyFrame 3 index: " << keyFrames.at(2).frameBufferIndex << " features: " << keyFrames.at(2).currentFrameIndexes.size() << " pxlDelta: " << keyFrames.at(2).pixelDelta);
	//ROS_DEBUG_STREAM("KeyFrame 4 index: " << keyFrames.at(3).frameBufferIndex << " features: " << keyFrames.at(3).currentFrameIndexes.size() << " pxlDelta: " << keyFrames.at(3).pixelDelta);
	ROS_DEBUG_STREAM((ros::Time::now().toSec() - start.toSec()) * 1000 << " milliseconds runtime (key frame computation)");
}


/*
 * publishes all active points in the list using the publisher if the user has specified
 */
void VIO::publishActivePoints()
{
	if(this->PUBLISH_ACTIVE_FEATURES)
	{

		sensor_msgs::PointCloud pc;

		std::vector<geometry_msgs::Point32> point;
		std::vector<sensor_msgs::ChannelFloat32> colors;

		pc.header.frame_id = this->world_frame;

		for(int i = 0; i < this->active3DFeatures.size(); i++)
		{
			//debugFeature(this->active3DFeatures.at(i));

			std::vector<float> intensity;
			sensor_msgs::ChannelFloat32 c;

			intensity.push_back(this->active3DFeatures.at(i).color.val[0]);
			//intensity.push_back(this->active3DFeatures.at(i).color[1]);
			//intensity.push_back(this->active3DFeatures.at(i).color[2]);

			c.values = intensity;
			c.name = "intensity";

			geometry_msgs::Point32 pt;
			pt.x = this->active3DFeatures.at(i).position.x();
			pt.y = this->active3DFeatures.at(i).position.y();
			pt.z = this->active3DFeatures.at(i).position.z();

			point.push_back(pt);
			colors.push_back(c);

		}

		pc.points = point;
		pc.channels = colors;

		this->activePointsPub.publish(pc); // publish!
	}
}

struct less_than_key {
	inline bool operator()(const VIOFeature3D& ft1, const VIOFeature3D& ft2) {
		return (ft1.variance < ft2.variance);
	}
};

void VIO::sortActive3DFeaturesByVariance() {
	std::sort(active3DFeatures.begin(), active3DFeatures.end(),
			less_than_key());
}

void VIO::publishFeatureDepth()
{
	if(this->PUBLISH_ACTIVE_FEATURES)
	{
		sensor_msgs::PointCloud pc;

		std::vector<geometry_msgs::Point32> point;
		std::vector<sensor_msgs::ChannelFloat32> colors;

		pc.header.frame_id = this->camera_frame;

		Frame& cf = currentFrame();

		for(auto& e : cf.features)
		{
			//debugFeature(this->active3DFeatures.at(i));

			std::vector<float> intensity;
			sensor_msgs::ChannelFloat32 c;

			intensity.push_back(255);
			//intensity.push_back(this->active3DFeatures.at(i).color[1]);
			//intensity.push_back(this->active3DFeatures.at(i).color[2]);

			c.values = intensity;
			c.name = "intensity";

			geometry_msgs::Point32 pt;
			pt.x = e.getFeatureDepth() * e.getUndistorted().x;
			pt.y = e.getFeatureDepth() * e.getUndistorted().y;
			pt.z = e.getFeatureDepth();

			point.push_back(pt);
			colors.push_back(c);

		}

		pc.points = point;
		pc.channels = colors;

		this->featureDepthPub.publish(pc);
	}
}


// OLD FOR REFERENCE

/*double VIO::recoverPoseV2( cv::InputArray E, cv::InputArray _points1, cv::InputArray _points2, cv::InputArray _cameraMatrix,
		cv::OutputArray _R, cv::OutputArray _t, cv::InputOutputArray _mask, VIOState x1, VIOState x2)
{

	cv::Mat points1, points2, cameraMatrix;
	_points1.getMat().convertTo(points1, CV_64F);
	_points2.getMat().convertTo(points2, CV_64F);
	_cameraMatrix.getMat().convertTo(cameraMatrix, CV_64F);

	int npoints = points1.checkVector(2);
	CV_Assert( npoints >= 0 && points2.checkVector(2) == npoints &&
			points1.type() == points2.type());

	CV_Assert(cameraMatrix.rows == 3 && cameraMatrix.cols == 3 && cameraMatrix.channels() == 1);

	if (points1.channels() > 1)
	{
		points1 = points1.reshape(1, npoints);
		points2 = points2.reshape(1, npoints);
	}

	double fx = cameraMatrix.at<double>(0,0);
	double fy = cameraMatrix.at<double>(1,1);
	double cx = cameraMatrix.at<double>(0,2);
	double cy = cameraMatrix.at<double>(1,2);

	points1.col(0) = (points1.col(0) - cx) / fx;
	points2.col(0) = (points2.col(0) - cx) / fx;
	points1.col(1) = (points1.col(1) - cy) / fy;
	points2.col(1) = (points2.col(1) - cy) / fy;

	points1 = points1.t();
	points2 = points2.t();

	cv::Mat R1, R2, t;
	cv::decomposeEssentialMat(E, R1, R2, t);
	cv::Mat P0 = cv::Mat::eye(3, 4, R1.type());
	cv::Mat P1(3, 4, R1.type()), P2(3, 4, R1.type()), P3(3, 4, R1.type()), P4(3, 4, R1.type());
	P1(cv::Range::all(), cv::Range(0, 3)) = R1 * 1.0; P1.col(3) = t * 1.0;
	P2(cv::Range::all(), cv::Range(0, 3)) = R2 * 1.0; P2.col(3) = t * 1.0;
	P3(cv::Range::all(), cv::Range(0, 3)) = R1 * 1.0; P3.col(3) = -t * 1.0;
	P4(cv::Range::all(), cv::Range(0, 3)) = R2 * 1.0; P4.col(3) = -t * 1.0;

	// Do the cheirality check.
	// Notice here a threshold dist is used to filter
	// out far away points (i.e. infinite points) since
	// there depth may vary between postive and negtive.
	double dist = 50.0;
	cv::Mat Q;
	cv::triangulatePoints(P0, P1, points1, points2, Q);
	cv::Mat mask1 = Q.row(2).mul(Q.row(3)) > 0;
	Q.row(0) /= Q.row(3);
	Q.row(1) /= Q.row(3);
	Q.row(2) /= Q.row(3);
	Q.row(3) /= Q.row(3);
	mask1 = (Q.row(2) < dist) & mask1;
	Q = P1 * Q;
	mask1 = (Q.row(2) > 0) & mask1;
	mask1 = (Q.row(2) < dist) & mask1;

	cv::triangulatePoints(P0, P2, points1, points2, Q);
	cv::Mat mask2 = Q.row(2).mul(Q.row(3)) > 0;
	Q.row(0) /= Q.row(3);
	Q.row(1) /= Q.row(3);
	Q.row(2) /= Q.row(3);
	Q.row(3) /= Q.row(3);
	mask2 = (Q.row(2) < dist) & mask2;
	Q = P2 * Q;
	mask2 = (Q.row(2) > 0) & mask2;
	mask2 = (Q.row(2) < dist) & mask2;

	cv::triangulatePoints(P0, P3, points1, points2, Q);
	cv::Mat mask3 = Q.row(2).mul(Q.row(3)) > 0;
	Q.row(0) /= Q.row(3);
	Q.row(1) /= Q.row(3);
	Q.row(2) /= Q.row(3);
	Q.row(3) /= Q.row(3);
	mask3 = (Q.row(2) < dist) & mask3;
	Q = P3 * Q;
	mask3 = (Q.row(2) > 0) & mask3;
	mask3 = (Q.row(2) < dist) & mask3;

	cv::triangulatePoints(P0, P4, points1, points2, Q);
	cv::Mat mask4 = Q.row(2).mul(Q.row(3)) > 0;
	Q.row(0) /= Q.row(3);
	Q.row(1) /= Q.row(3);
	Q.row(2) /= Q.row(3);
	Q.row(3) /= Q.row(3);
	mask4 = (Q.row(2) < dist) & mask4;
	Q = P4 * Q;
	mask4 = (Q.row(2) > 0) & mask4;
	mask4 = (Q.row(2) < dist) & mask4;

	mask1 = mask1.t();
	mask2 = mask2.t();
	mask3 = mask3.t();
	mask4 = mask4.t();

	// If _mask is given, then use it to filter outliers.
	if (!_mask.empty())
	{
		cv::Mat mask = _mask.getMat();
		CV_Assert(mask.size() == mask1.size());
		cv::bitwise_and(mask, mask1, mask1);
		cv::bitwise_and(mask, mask2, mask2);
		cv::bitwise_and(mask, mask3, mask3);
		cv::bitwise_and(mask, mask4, mask4);
	}
	if (_mask.empty() && _mask.needed())
	{
		_mask.create(mask1.size(), CV_8U);
	}

	CV_Assert(_R.needed() && _t.needed());
	_R.create(3, 3, R1.type());
	_t.create(3, 1, t.type());

	double chProb1 = (double)countNonZero(mask1) / (double)npoints;
	double chProb2 = (double)countNonZero(mask2) / (double)npoints;
	double chProb3 = (double)countNonZero(mask3) / (double)npoints;
	double chProb4 = (double)countNonZero(mask4) / (double)npoints;

	//1 - R1 t
	//2 - R2 t
	//3 - R1 -t
	//4 - R2 -t

	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException& e){
		ROS_WARN_STREAM(e.what());
	}

	tf::Transform tf_cam1 = tf::Transform(x1.getTFQuaternion(), tf::Vector3(x1.x(), x1.y(), x1.z())) * base2cam;
	tf::Transform tf_cam2 = tf::Transform(x2.getTFQuaternion(), tf::Vector3(x2.x(), x2.y(), x2.z())) * base2cam;

	tf::Transform tf_imu = (tf_cam1.inverse() * tf_cam2).inverse();

	tf::Vector3 t_imu = tf_imu.getOrigin().normalized();
	tf::Quaternion q_imu = tf_imu.getRotation();
	tf::Vector3 t_ego = tf::Vector3(t.at<double>(0), t.at<double>(1), t.at<double>(2));

	//r1 to q_r1;
	Eigen::Matrix3f eig_R1, eig_R2;
	Eigen::Quaternionf eig_R1_q, eig_R2_q;
	cv::cv2eigen(R1, eig_R1);
	cv::cv2eigen(R2, eig_R2);
	eig_R1_q = eig_R1;
	eig_R2_q = eig_R2;
	tf::Quaternion R1_q = tf::Quaternion(eig_R1_q.x(), eig_R1_q.y(), eig_R1_q.z(), eig_R1_q.w());
	tf::Quaternion R2_q = tf::Quaternion(eig_R2_q.x(), eig_R2_q.y(), eig_R2_q.z(), eig_R2_q.w());

	double negTProb = (t_imu.dot(-1 * t_ego) + 1.0) / 2.0;
	double posTProb = (t_imu.dot(t_ego) + 1.0) / 2.0;
	double R1Prob = q_imu.angleShortestPath(R1_q) / CV_PI;
	double R2Prob = q_imu.angleShortestPath(R2_q) / CV_PI;

	ROS_DEBUG_STREAM("R1: " << R1Prob << " R2: " << R2Prob);
	ROS_DEBUG_STREAM("t: " << posTProb << " -t: " << negTProb);
	ROS_DEBUG_STREAM("t: " << t.at<double>(0) << ", " << t.at<double>(1) << ", " << t.at<double>(2));

	double imuProb1 = 0.6 * R1Prob + 0.4 * posTProb;
	double imuProb2 = 0.6 * R2Prob + 0.4 * posTProb;
	double imuProb3 = 0.6 * R1Prob + 0.4 * negTProb;
	double imuProb4 = 0.6 * R2Prob + 0.4 * negTProb;
	ROS_ASSERT(imuProb1 >= 0 && imuProb1 <= 1 && imuProb2 >= 0 && imuProb2 <= 1);

	double prob1 = 0.3 * imuProb1 + 0.7 * chProb1;
	double prob2 = 0.3 * imuProb2 + 0.7 * chProb2;
	double prob3 = 0.3 * imuProb3 + 0.7 * chProb3;
	double prob4 = 0.3 * imuProb4 + 0.7 * chProb4;

	//finalize and return;
	if(prob1 > prob2 && prob1 > prob3 && prob1 > prob4)
	{
		R1.copyTo(_R);
		t = tf_imu.getOrigin().length() * t;
		t.copyTo(_t);
		ROS_DEBUG_STREAM("R1 & t chosen");
		if (_mask.needed()) mask1.copyTo(_mask);
		return prob1;
	}
	else if(prob2 > prob1 && prob2 > prob3 && prob2 > prob4)
	{
		R2.copyTo(_R);
		t = tf_imu.getOrigin().length() * t;
		t.copyTo(_t);
		ROS_DEBUG_STREAM("R2 & t chosen");
		if (_mask.needed()) mask2.copyTo(_mask);
		return prob2;
	}
	else if(prob3 > prob2 && prob3 > prob1 && prob3 > prob4)
	{
		R1.copyTo(_R);
		t = -1 * tf_imu.getOrigin().length() * t;
		t.copyTo(_t);
		ROS_DEBUG_STREAM("R1 & -t chosen");
		if (_mask.needed()) mask3.copyTo(_mask);
		return prob3;
	}
	else
	{
		R2.copyTo(_R);
		t = -1 * tf_imu.getOrigin().length() * t;
		t.copyTo(_t);
		ROS_DEBUG_STREAM("R2 & -t chosen");
		if (_mask.needed()) mask4.copyTo(_mask);
		return prob4;
	}



}



double VIO::poseFromPoints(std::vector<VIOFeature3D> actives, Frame lf, Frame cf, Eigen::Matrix<double, 7, 1>& Z, bool& pass)
{
	if(actives.size() < 3)
	{
		pass = false;
		return DBL_MAX;
	}

	std::vector<cv::Point3f> objectPoints;
	std::vector<cv::Point2f> imagePoints;

	cv::Mat tvec, rvec;

	double cov_sum = 0;
	int totalMatches = 0;

	for(int i = 0; i < actives.size(); i++)
	{
		if(lf.features.at(actives.at(i).current2DFeatureMatchIndex).forwardMatched)
		{
			objectPoints.push_back(cv::Point3f(actives.at(i).position(0), actives.at(i).position(1), actives.at(i).position(2)));

			VIOFeature2D pt = cf.features.at(lf.features.at(actives.at(i).current2DFeatureMatchIndex).forwardMatchIndex);

			ROS_ASSERT(pt.getFeatureID() == lf.features.at(actives.at(i).current2DFeatureMatchIndex).forwardMatchID);
			ROS_ASSERT(pt.getMatchedID() == lf.features.at(actives.at(i).current2DFeatureMatchIndex).getFeatureID());
			ROS_ASSERT(actives.at(i).current2DFeatureMatchID = lf.features.at(actives.at(i).current2DFeatureMatchIndex).getFeatureID());
			ROS_ASSERT(pt.getMatchedID() == actives.at(i).current2DFeatureMatchID);

			imagePoints.push_back(pt.getUndistorted(true));

			ROS_DEBUG_STREAM("3D Point: " << cv::Point3f(actives.at(i).position(0), actives.at(i).position(1), actives.at(i).position(2)) << " \nCorresponding to: " << pt.getFeaturePosition()
					<< "\nwith cov: " << actives.at(i).variance << "\n");

			cov_sum += actives.at(i).variance;
			totalMatches++;
		}
	}

	if(objectPoints.size() < 3)
	{
		pass = false;
		return DBL_MAX;
	}

	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException& e){
		ROS_WARN_STREAM(e.what());
	}

	float reprojError;
	cv::Mat inliers;
	//cv::Mat::eye(cv::Size(3, 3), CV_32F)
	//cv::solvePnP(objectPoints, imagePoints, cv::Mat::eye(cv::Size(3, 3), CV_32F), cv::noArray(), rvec, tvec, false, cv::SOLVEPNP_P3P);
	cv::solvePnPRansac(objectPoints, imagePoints, cv::Mat::eye(cv::Size(3, 3), CV_32F), cv::noArray(), rvec, tvec, false, 100, 0.5, 0.99, inliers, cv::SOLVEPNP_P3P);

	cv::Mat cv_R;

	cv::Rodrigues(rvec, cv_R);

	tf::Vector3 t = tf::Vector3(tvec.at<float>(0), tvec.at<float>(1), tvec.at<float>(2));

	Eigen::Matrix<float, 3, 3> R;
	cv::cv2eigen(cv_R, R);

	Eigen::Quaternionf q_eig(R.transpose());
	tf::Quaternion q = tf::Quaternion(q_eig.x(), q_eig.y(), q_eig.z(), q_eig.w());

	tf::Transform world2base = tf::Transform(q, t).inverse() * base2cam.inverse();

	Z(0, 0) = world2base.getOrigin().x();
	Z(1, 0) = world2base.getOrigin().y();
	Z(2, 0) = world2base.getOrigin().z();

	tf::Quaternion newQ = world2base.getRotation();

	Z(3, 0) = newQ.w();
	Z(4, 0) = newQ.x();
	Z(5, 0) = newQ.y();
	Z(6, 0) = newQ.z();

	ROS_DEBUG_STREAM("PNP: r: " << Z(0) << ", " << Z(1) << ", " << Z(2) << " q: " << Z(3) << ", " << Z(4) << ", " << Z(5) << ", " << Z(6));
	ROS_DEBUG_STREAM("PNP COV: " << cov_sum / totalMatches);

	pass = true;
	return cov_sum / totalMatches;
}*/





// -=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=
//LEGACY - Version 2

/*



 * computes the F mat
 * gets scales translation


double VIO::computeFundamentalMatrix(cv::Mat& F, cv::Matx33f& R, cv::Matx31f& t, std::vector<cv::Point2f>& pt1_out, std::vector<cv::Point2f>& pt2_out, bool& pass, std::vector<VIOFeature2D>& ft1_out, std::vector<VIOFeature2D>& ft2_out, int& match_frame_index)
{
	double pixel_delta = 0;
	VIOState x1, x2;
	std::vector<VIOFeature2D> ft1, ft2;
	int match_index;

	double error = 0;

	//std::vector<cv::Point2f> pt1_new, pt2_new;
	std::vector<cv::Point2f> pt1, pt2;

	this->getBestCorrespondences(pixel_delta, ft1, ft2, x1, x2, match_index);

	match_frame_index = match_index;

	if(pixel_delta > MIN_FUNDAMENTAL_PXL_DELTA)
	{
		pass = true;

		for(int i = 0; i < ft1.size(); i++)
		{
			pt1.push_back(ft1.at(i).getUndistorted());
			pt2.push_back(ft2.at(i).getUndistorted());
		}

		cv::Mat mask;
		cv::Mat E = cv::findEssentialMat(pt1, pt2, cv::Mat::eye(cv::Size(3, 3), CV_32F), cv::RANSAC, 0.999, 1.0, mask);

		//std::vector<cv::Point2f> pt1_copy, pt2_copy;
		//pt1_copy = pt1;
		//pt2_copy = pt2;
		//cv::correctMatches(E, pt1_copy, pt2_copy, pt1_new, pt2_new);

		//pt1_new = pt1;
		//pt2_new = pt2;

		for(int i = 0; i < pt1.size(); i++)
		{
			cv::Matx31f u1, u2;

			u1(0) = pt1.at(i).x;
			u1(1) = pt1.at(i).y;
			u1(2) = 1.0;

			u2(0) = pt2.at(i).x;
			u2(1) = pt2.at(i).y;
			u2(2) = 1.0;

			Eigen::Matrix<float, 3, 1> u1_eig, u2_eig;
			Eigen::Matrix<float, 3, 3> E_eig;

			cv::cv2eigen(u1, u1_eig);
			cv::cv2eigen(u2, u2_eig);
			cv::cv2eigen(E, E_eig);

			error += abs((u2_eig.transpose() * E_eig * u1_eig)(0, 0));

		}
		//error = 0;

		cv::Mat R_temp, t_hat;

		double goodProb = this->recoverPoseV2(E, pt1, pt2, cv::Mat::eye(cv::Size(3, 3), CV_32F), R_temp, t_hat, mask, frameBuffer.at(match_index).state, currentFrame().state);
		//int goodPoints = cv::recoverPose(E, pt1, pt2, cv::Mat::eye(cv::Size(3, 3), CV_32F), R_temp, t_hat, mask);

		ROS_DEBUG_STREAM("error: " << error);

		R_temp.convertTo(R_temp,  R.type);
		R_temp.copyTo(R);
		t_hat.convertTo(t_hat,  t.type);
		t_hat.copyTo(t);

		//t = ((currentFrame().state.getr() - frameBuffer.at(match_index).state.getr()).norm() * t); // return the scaled translation using the two frames states

		ft1_out = ft1;
		ft2_out = ft2;

		pt1_out = pt1;
		pt2_out = pt2;

		F = E;

		ROS_DEBUG_STREAM("tFinal: " << t);

	}
	else
	{
		ROS_DEBUG_STREAM("pixel delta too small for fundamental mat computation");
		error = 1e9;
		pass = false;
	}

	this->viewMatches(ft1, ft2, this->frameBuffer.at(match_index), currentFrame(), pt1, pt2);

	return error;
}


void VIO::getBestCorrespondences(double& pixel_delta, std::vector<VIOFeature2D>& ft1, std::vector<VIOFeature2D>& ft2, VIOState& x1, VIOState& x2, int& match_index)
{
	ros::Time start = ros::Time::now();

	int bestFtIndex = 0;
	match_index = 0;

	double bestPxlDelta = 0;

	std::deque<std::deque<VIOFeature2D> > temp_ft1;
	std::deque<std::deque<VIOFeature2D> > temp_ft2;

	std::deque<VIOFeature2D> dq;
	for(int i = 0; i < currentFrame().features.size(); i++)
		dq.push_back(currentFrame().features.at(i));

	temp_ft1.push_back(dq);
	temp_ft2.push_back(dq);

	for(int i = 1; i < this->frameBuffer.size(); i++)
	{
		float pxlSum = 0;

		//ROS_DEBUG("t0");

		//ROS_DEBUG_STREAM("last vec size: " << temp_ft1.at(i - 1).size());

		std::deque<VIOFeature2D> t1, t2;

		temp_ft1.push_back(t1);
		temp_ft2.push_back(t2);

		//ros::Time start2 = ros::Time::now();

		for(int j = 0; j < temp_ft1.at(i - 1).size(); j++)
		{
			if(temp_ft1.at(i - 1).at(j).isMatched())
			{
				temp_ft2.at(i).push_back(temp_ft2.at(i - 1).at(j));

				temp_ft1.at(i).push_back(this->getCorrespondingFeature(temp_ft1.at(i - 1).at(j), this->frameBuffer.at(i)));

				pxlSum += this->manhattan(temp_ft1.at(i).at(temp_ft1[i].size() - 1).getUndistorted(), temp_ft2.at(i - 1).at(j).getUndistorted());

			}
		}

		//pxlSum = i;

		//ROS_DEBUG_STREAM((ros::Time::now().toSec() - start2.toSec()) * 1000 << " milliseconds runtime (feature correspondence inner loop)");

		//ROS_DEBUG("t1");

		if(temp_ft1.at(i).size() < MIN_TRIAG_FEATURES)
		{
			//ROS_DEBUG_STREAM("too few features break");
			break;
		}

		if(pxlSum / temp_ft1.at(i).size() > IDEAL_FUNDAMENTAL_PXL_DELTA)
		{
			//ROS_DEBUG_STREAM("ideal pxl delta break");

			bestFtIndex = i;

			bestPxlDelta = pxlSum / temp_ft1.at(i).size();

			match_index = i;
			break;
		}

		if(pxlSum / temp_ft1.size() > bestPxlDelta)
		{
			//ROS_DEBUG_STREAM("better pxlDelta");

			bestFtIndex = i;

			bestPxlDelta = pxlSum / temp_ft1.at(i).size();

			match_index = i;
		}

		//ROS_DEBUG("t2");

	}

	std::copy(temp_ft1.at(bestFtIndex).begin(), temp_ft1.at(bestFtIndex).end(), std::back_inserter(ft1));
	std::copy(temp_ft2.at(bestFtIndex).begin(), temp_ft2.at(bestFtIndex).end(), std::back_inserter(ft2));
	x1 = frameBuffer.at(bestFtIndex).state;
	x2 = frameBuffer.at(0).state;
	pixel_delta = bestPxlDelta;

	ROS_DEBUG_STREAM("best pixel delta " << pixel_delta);
	ROS_DEBUG_STREAM("match index: " << match_index);
	ROS_DEBUG_STREAM((ros::Time::now().toSec() - start.toSec()) * 1000 << " milliseconds runtime (feature correspondence)");
}

 */


//-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
//LEGACY - Version 1

/*


 * uses epipolar geometry from two frames to
 * estimate relative motion of the frame;

bool VIO::visualMotionInference(Frame frame1, Frame frame2, tf::Vector3 angleChangePrediction,
		tf::Vector3& rotationInference, tf::Vector3& unitVelocityInference, double& averageMovement)
{
	//first get the feature deltas from the two frames
	std::vector<cv::Point2f> prevPoints, currentPoints;
	feature_tracker.getCorrespondingPointsFromFrames(frame1, frame2, prevPoints, currentPoints);

	//undistort points using fisheye model
	//cv::fisheye::undistortPoints(prevPoints, prevPoints, this->K, this->D);
	//cv::fisheye::undistortPoints(currentPoints, currentPoints, this->K, this->D);

	//get average movement bewteen images
	averageMovement = feature_tracker.averageFeatureChange(prevPoints, currentPoints);

	//ensure that there are enough points to estimate motion with vo
	if(currentPoints.size() < 5)
	{
		return false;
	}

	cv::Mat mask;

	//calculate the essential matrix
	cv::Mat essentialMatrix = cv::findEssentialMat(prevPoints, currentPoints, this->K, cv::RANSAC, 0.999, 1.0, mask);

	//ensure that the essential matrix is the correct size
	if(essentialMatrix.rows != 3 || essentialMatrix.cols != 3)
	{
		return false;
	}

	//recover pose change from essential matrix
	cv::Mat translation;
	cv::Mat rotation;

	//decompose matrix to get possible deltas
	cv::recoverPose(essentialMatrix, prevPoints, currentPoints, this->K, rotation, translation, mask);


	//set the unit velocity inference
	unitVelocityInference.setX(translation.at<double>(0, 0));
	unitVelocityInference.setY(translation.at<double>(1, 0));
	unitVelocityInference.setZ(translation.at<double>(2, 0));

	return true;
}

 *
 From "Triangulation", Hartley, R.I. and Sturm, P., Computer vision and image understanding, 1997

cv::Mat_<double> VIO::IterativeLinearLSTriangulation(cv::Point3d u,    //homogenous image point (u,v,1)
		cv::Matx34d P,          //camera 1 matrix
		cv::Point3d u1,         //homogenous image point in 2nd camera
		cv::Matx34d P1          //camera 2 matrix
) {
	//double error;
	double wi = 1, wi1 = 1;
	cv::Mat_<double> X(4,1);
	for (int i=0; i<10; i++) { //Hartley suggests 10 iterations at most
		cv::Mat_<double> X_ = this->LinearLSTriangulation(u,P,u1,P1);
		X(0) = X_(0); X(1) = X_(1); X(2) = X_(2);
		X(3) = X_(3);

		//recalculate weights
		double p2x = cv::Mat_<double>(cv::Mat_<double>(P).row(2)*X)(0);
		double p2x1 = cv::Mat_<double>(cv::Mat_<double>(P1).row(2)*X)(0);

		//breaking point
		//error = fabsf(wi - p2x) + fabsf(wi1 - p2x1);
		if(fabsf(wi - p2x) <= 1.0 && fabsf(wi1 - p2x1) <= 1.0) break;

		wi = p2x;
		wi1 = p2x1;

		//reweight equations and solve
		cv::Matx43d A((u.x*P(2,0)-P(0,0))/wi,       (u.x*P(2,1)-P(0,1))/wi,         (u.x*P(2,2)-P(0,2))/wi,
				(u.y*P(2,0)-P(1,0))/wi,       (u.y*P(2,1)-P(1,1))/wi,         (u.y*P(2,2)-P(1,2))/wi,
				(u1.x*P1(2,0)-P1(0,0))/wi1,   (u1.x*P1(2,1)-P1(0,1))/wi1,     (u1.x*P1(2,2)-P1(0,2))/wi1,
				(u1.y*P1(2,0)-P1(1,0))/wi1,   (u1.y*P1(2,1)-P1(1,1))/wi1,     (u1.y*P1(2,2)-P1(1,2))/wi1
		);
		cv::Mat_<double> B = (cv::Mat_<double>(4,1) <<    -(u.x*P(2,3)    -P(0,3))/wi,
				-(u.y*P(2,3)  -P(1,3))/wi,
				-(u1.x*P1(2,3)    -P1(0,3))/wi1,
				-(u1.y*P1(2,3)    -P1(1,3))/wi1
		);

		cv::solve(A,B,X_,cv::DECOMP_SVD);
		X(0) = X_(0); X(1) = X_(1); X(2) = X_(2);
		X(3) = X_(3);
	}

	//ROS_DEBUG_STREAM("triag error: " << error);
	return X;
}

 *
 From "Triangulation", Hartley, R.I. and Sturm, P., Computer vision and image understanding, 1997

cv::Mat_<double> VIO::LinearLSTriangulation(cv::Point3d u,       //homogenous image point (u,v,1)
		cv::Matx34d P,       //camera 1 matrix
		cv::Point3d u1,      //homogenous image point in 2nd camera
		cv::Matx34d P1       //camera 2 matrix
)
{
	//build matrix A for homogenous equation system Ax = 0
	//assume X = (x,y,z,1), for Linear-LS method
	//which turns it into a AX = B system, where A is 4x3, X is 3x1 and B is 4x1
	cv::Matx43d A(u.x*P(2,0)-P(0,0),    u.x*P(2,1)-P(0,1),      u.x*P(2,2)-P(0,2),
			u.y*P(2,0)-P(1,0),    u.y*P(2,1)-P(1,1),      u.y*P(2,2)-P(1,2),
			u1.x*P1(2,0)-P1(0,0), u1.x*P1(2,1)-P1(0,1),   u1.x*P1(2,2)-P1(0,2),
			u1.y*P1(2,0)-P1(1,0), u1.y*P1(2,1)-P1(1,1),   u1.y*P1(2,2)-P1(1,2)
	);
	cv::Mat_<double> B = (cv::Mat_<double>(4,1) <<    -(u.x*P(2,3)    -P(0,3)),
			-(u.y*P(2,3)  -P(1,3)),
			-(u1.x*P1(2,3)    -P1(0,3)),
			-(u1.y*P1(2,3)    -P1(1,3)));

	cv::Mat_<double> X;
	cv::solve(A,B,X, cv::DECOMP_SVD);

	return X;
}



 * triangulates 3d points and reprojects them giving an error
 * if false the point is invalid

bool VIO::triangulateAndCheck(cv::Point2f pt1, cv::Point2f pt2, cv::Matx33d K1, cv::Matx33d K2, VIOState x1_b, VIOState x2_b, double& error, cv::Matx31d& r, tf::Transform base2cam)
{

	VIOState x1_c, x2_c;

	Eigen::Quaterniond q1_b = Eigen::Quaterniond(x1_b.q1(), x1_b.q2(), x1_b.q3(), x1_b.q0());
	Eigen::Quaterniond q2_b = Eigen::Quaterniond(x2_b.q1(), x2_b.q2(), x2_b.q3(), x2_b.q0());

	tf::Quaternion temp_q = base2cam.getRotation();
	Eigen::Quaterniond q_b_c = Eigen::Quaterniond(temp_q.getX(), temp_q.getY(), temp_q.getZ(), temp_q.getW());

	base2cam * base2cam;

	Eigen::Quaterniond q1_c = q1_b * q_b_c;
	Eigen::Quaterniond q2_c = q2_b * q_b_c;

	//q1 * diff = q2 => inv(q1)* q2 = diff
	Eigen::Quaterniond diff = q1_c.inverse() * q2_c; // the relative rotation quaternion

	Eigen::Vector3d r1_b = Eigen::Vector3d(x1_b.x(), x1_b.y(), x1_b.z());
	Eigen::Vector3d r2_b = Eigen::Vector3d(x2_b.x(), x2_b.y(), x2_b.z());

	Eigen::Vector3d r_b_c = Eigen::Vector3d(base2cam.getOrigin().getX(), base2cam.getOrigin().getY(), base2cam.getOrigin().getZ());

	Eigen::Vector3d r1_c = (q1_b * r_b_c + r1_b);
	Eigen::Matrix<double, 3, 1> dr = (q2_b * r_b_c + r2_b) - r1_c;

	cv::Matx34d P1;
	cv::hconcat(cv::Mat::eye(3, 3, CV_64F), cv::Mat::zeros(3, 1, CV_64F), P1);

	//this creates a transformation from the first camera to the second
	Eigen::Matrix<double, 3, 3> R_ = diff.inverse().toRotationMatrix();
	Eigen::Matrix<double, 3, 1> t_ = R_ * -dr;

	cv::Mat R_cv, t_cv;
	cv::eigen2cv(R_, R_cv);
	cv::eigen2cv(t_, t_cv);

	cv::Matx34d P2;
	cv::hconcat(R_cv, t_cv, P2);

	//ROS_DEBUG_STREAM("P1: " << P1);
	//ROS_DEBUG_STREAM("P2: " << P2);
	//now P1 and P2 are made
	//results will be in camera 1 coordinate system

	//using tf to compute P2

	tf::Quaternion q1_b, q2_b;
	q1_b = tf::Quaternion(x1_b.q1(), x1_b.q2(), x1_b.q3(), x1_b.q0());
	q2_b = tf::Quaternion(x2_b.q1(), x2_b.q2(), x2_b.q3(), x2_b.q0());

	tf::Vector3 r1_b, r2_b;
	r1_b = tf::Vector3(x1_b.x(), x1_b.y(), x1_b.z());
	r2_b = tf::Vector3(x2_b.x(), x2_b.y(), x2_b.z());

	tf::Transform t1_b, t2_b;
	t1_b = tf::Transform(q1_b, r1_b);
	t2_b = tf::Transform(q2_b, r2_b);

	tf::Transform t1_c, t2_c;
	t1_c = t1_b * base2cam;
	t2_c = t2_b * base2cam;

	tf::Quaternion q1_c, q2_c;
	q1_c = t1_c.getRotation();
	q2_c = t2_c.getRotation();

	//q1 * diff = q2 => diff = q1.inv * q2

	tf::Transform temp = tf::Transform(q1_c.inverse() * q2_c, t2_c.getOrigin() - t1_c.getOrigin());
	tf::Transform tf_p2 = temp.inverse();

	cv::Matx34d P2;
	P2(0, 0) = tf_p2.getBasis().getRow(0).x();
	P2(0, 1) = tf_p2.getBasis().getRow(0).y();
	P2(0, 2) = tf_p2.getBasis().getRow(0).z();
	P2(1, 0) = tf_p2.getBasis().getRow(1).x();
	P2(1, 1) = tf_p2.getBasis().getRow(1).y();
	P2(1, 2) = tf_p2.getBasis().getRow(1).z();
	P2(2, 0) = tf_p2.getBasis().getRow(2).x();
	P2(2, 1) = tf_p2.getBasis().getRow(2).y();
	P2(2, 2) = tf_p2.getBasis().getRow(2).z();

	P2(0, 3) = tf_p2.getOrigin().x();
	P2(1, 3) = tf_p2.getOrigin().y();
	P2(2, 3) = tf_p2.getOrigin().z();

	//break if not moved enough

	double d = sqrt(P2.col(3).dot(P2.col(3)));
	if(d < MIN_TRIANGUALTION_DIST){
		ROS_DEBUG_STREAM("d too small: " << d);
		return false;
	}

	//ROS_DEBUG_STREAM(P2);

	cv::Matx34d P1;
	cv::hconcat(cv::Mat::eye(3, 3, CV_64F), cv::Mat::zeros(3, 1, CV_64F), P1);


	cv::Matx41d X;
	cv::Matx61d b;
	cv::Mat_<double> A = cv::Mat_<double>(6, 4);

	b(0) = pt1.x;
	b(1) = pt1.y;
	b(2) = 1.0;
	b(3) = pt2.x;
	b(4) = pt2.y;
	b(5) = 1.0;

	cv::vconcat(K1 * P1, K2 * P2, A);

	//ROS_DEBUG_STREAM("K1: " << K1);
	//ROS_DEBUG_STREAM("K2: " << K2);
	//ROS_DEBUG_STREAM("A: " << A);
	//ROS_DEBUG_STREAM("b: " << b);

	//now we can triangulate

	cv::solve(A, b, X, cv::DECOMP_SVD);

	r(0) = X(0) / X(3);
	r(1) = X(1) / X(3);
	r(2) = X(2) / X(3);

	X(0) = r(0);
	X(1) = r(1);
	X(2) = r(2);
	X(3) = r(3);

	//reproject

	cv::Matx31d b1 = K1 * P1 * X;
	b1(0) = b1(0) / b1(2);
	b1(1) = b1(1) / b1(2);
	b1(2) = 1.0;

	cv::Matx31d b2 = K2 * P2 * X;
	b2(0) = b2(0) / b2(2);
	b2(1) = b2(1) / b2(2);
	b2(2) = 1.0;

	cv::Matx61d b_;
	cv::vconcat(b1, b2, b_);

	error = (b_ - b).dot(b_ - b);

	if(r(2) > MIN_TRIAG_Z && error < MAX_TRIAG_ERROR)
	{
		ROS_DEBUG_STREAM("r: " << r);
		ROS_DEBUG_STREAM("error: " << error);
		ROS_DEBUG_STREAM("pt: " << pt2);
		//ROS_DEBUG_STREAM("du: " << (b_ - b).t());
		//transform the point into world coordinates

		tf::Vector3 tf_r = tf::Vector3(r(0), r(1), r(2));

		tf::Vector3 tf_r_w = t1_c.inverse() * tf_r;

		r(0) = tf_r_w.getX();
		r(1) = tf_r_w.getY();
		r(2) = tf_r_w.getZ();

		ROS_DEBUG_STREAM("r world: " << r);
		return true;
	}
	else
	{
		return false;
	}
}


 * this function uses the oldest state possible and the current state
 * along with the previous frame and current frame to
 * update each 3d feature and add new 3d features if necessary
 * If 3d feature is not updated it will be either removed or added to the inactive list.

void VIO::update3DFeatures(VIOState x, VIOState x_last, Frame cf, Frame lf, std::deque<Frame> fb)
{
	std::vector<VIOFeature3D> inactives = this->active3DFeatures; // set the new inactive features to be the current active features
	std::vector<VIOFeature3D> actives;

	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException e){
		ROS_WARN_STREAM(e.what());
	}


	for(int i = 0; i < cf.features.size(); i++)
	{
		if(cf.features.at(i).isMatched()) // if this feature is matched
		{

			// store the currentFeature and lastFeature
			VIOFeature2D current2DFeature = cf.features.at(i);
			int frame_index = 0;
			VIOFeature2D last2DFeature;
			this->findBestCorresponding2DFeature(current2DFeature, lf, fb, last2DFeature, frame_index); // finds the best corresponding 2d feature

			//set up each of the relavant matrices depending on the frame which the oldest
			// corresponding feature lies in
			cv::Matx33d K1;
			cv::Matx33d K2 = currentFrame.K;
			if(frame_index == -1)
			{
				K1 = this->lastFrame.K;
			}
			else
			{
				K1 = fb.at(frame_index).K;
			}

			//ROS_DEBUG_STREAM(frame_index);
			//ROS_DEBUG_STREAM("P1: " << P1.col(3) << "\nP2: " << P2.col(3));

			//check if this feature has a matched 3d feature
			bool match3D = false;
			VIOFeature3D matched3DFeature;
			for(int j = 0; j < inactives.size(); j++)
			{
				if(inactives.at(j).current2DFeatureMatchIndex == current2DFeature.getMatchedIndex())
				{
					matched3DFeature = inactives.at(j); // found a matched feature

					ROS_ASSERT(matched3DFeature.current2DFeatureMatchID == lf.features.at(current2DFeature.getMatchedIndex()).getFeatureID()); // ensure that everything matches up

					match3D = true; //set the flag for later

					inactives.erase(inactives.begin() + j); // erase the jth feature from inactives

					break;
				}
			}

			//TRIANGULATION

			VIOState x1, x2;
			x2 = this->state;
			if(frame_index == -1)
				x1 = this->lastState;
			else
				x1 = fb.at(frame_index).state;
			//ROS_DEBUG_STREAM("x1: " << x1.vector.transpose());
			//ROS_DEBUG_STREAM("x2: " << x2.vector.transpose());
			//ROS_DEBUG_STREAM("pt1: " << last2DFeature.getUndistorted());
			//ROS_DEBUG_STREAM("pt2: " << current2DFeature.getUndistorted() << "\n");



			cv::Matx31d r; //resulting point
			double error; //error in pixels

			bool successful = this->triangulateAndCheck(last2DFeature.getUndistorted(), current2DFeature.getUndistorted(), K1, K2, x1, x2, error, r, base2cam); // triangulate the points if possible

			if(successful)
			{
				double d = (x2.getr() - x1.getr()).norm();
				double r_cov_sum = (x1.covariance(0, 0) + x1.covariance(1, 1) + x1.covariance(2, 2) + x2.covariance(0, 0) + x2.covariance(1, 1) + x2.covariance(1, 1));

				ROS_DEBUG_STREAM("frame used: " << frame_index);
				ROS_DEBUG_STREAM("dist traveled: " << d);
				ROS_DEBUG_STREAM("r_cov_sum: " << r_cov_sum);

				if(match3D)
				{


					matched3DFeature.update(Eigen::Vector3d(r(0), r(1), r(2)), error + (1/d) + r_cov_sum);
					matched3DFeature.current2DFeatureMatchID = current2DFeature.getFeatureID();
					matched3DFeature.current2DFeatureMatchIndex = i;

					ROS_DEBUG_STREAM("updating feature new cov: " << matched3DFeature.variance);
					ROS_DEBUG_STREAM("after pos: " << matched3DFeature.position);

					actives.push_back(matched3DFeature);
				}
				else
				{


					ROS_DEBUG_STREAM("adding feature");
					VIOFeature3D newFeat;
					newFeat.color = cv::Scalar(255, 255, 255);
					newFeat.current2DFeatureMatchID = current2DFeature.getFeatureID();
					newFeat.current2DFeatureMatchIndex = i;
					newFeat.position = Eigen::Vector3d(r(0), r(1), r(2));
					newFeat.variance = 10000; // starting cov
					newFeat.colorSet = true;

					actives.push_back(newFeat);
				}
			}
			else if(match3D)
			{
				matched3DFeature.current2DFeatureMatchID = current2DFeature.getFeatureID();
				matched3DFeature.current2DFeatureMatchIndex = i;
				actives.push_back(matched3DFeature);
			}
		}
	}

	//set each of the 3d feature buffers to be published
	this->active3DFeatures = actives;
	this->inactive3DFeatures = inactives;
}

void VIO::findBestCorresponding2DFeature(VIOFeature2D start, Frame lf, std::deque<Frame> fb, VIOFeature2D& end, int& frameIndex)
{
	if(!start.isMatched()) ROS_ERROR("feature has no match!");


	VIOFeature2D temp = lf.features.at(start.getMatchedIndex());
	ROS_ASSERT(temp.getFeatureID() == start.getMatchedID());
	end = temp;

	//ROS_DEBUG_STREAM("fb size: " << fb.size());

	if(end.isMatched() && fb.size() > 0)
	{
		for(int i = 0; i < fb.size(); i++)
		{
			temp = fb.at(i).features.at(end.getMatchedIndex());
			ROS_ASSERT(temp.getFeatureID() == end.getMatchedID());
			end = temp;

			//ROS_DEBUG_STREAM("end match " << end.getMatchedID());
			//ROS_DEBUG_STREAM("this frame: " << fb.at(i).nextFeatureID);
			//ROS_DEBUG_STREAM("next frame: " << fb.at(i+1).nextFeatureID);

			if(!end.isMatched())
			{
				frameIndex = i;
				break;
			}
			frameIndex = i;
		}
	}
	else
	{
		frameIndex = -1;
	}

	//ROS_DEBUG_STREAM("frame index: " << frameIndex);
}



 */

 
 
/*

/*
 * NOTE: the position of the state, the last state, and the keyframe state must be estimated by this point
 *
 * this function cycles through all matched features from key frame 0,
 * transforms their depth into the current frame
 * calculates their depth in the current frame
 * updates their depth
 * if their depth variance is good enough
 * 	convert the point to a 3d features for future motion estimation
 *
 * x is the state of the current frame

void VIO::updateFeatureDepths(VIOState x, double variance)
{
	tf::StampedTransform base2cam;
	try{
		this->ekf.tf_listener.lookupTransform(this->camera_frame, this->CoM_frame, ros::Time(0), base2cam);
	}
	catch(tf::TransformException& e){
		ROS_WARN_STREAM(e.what());
	}

	Frame& cf = currentFrame();
	Frame& lf = lastFrame();
	KeyFrameInfo& kf = this->keyFrames.at(0);
	ROS_ASSERT(kf.nextFeatureID = this->frameBuffer.at(kf.frameBufferIndex).nextFeatureID);

	cv::Matx34d P1(1, 0, 0, 0,
			0, 1, 0, 0,
			0, 0, 1, 0);

	tf::Transform tf_current = this->cameraTransformFromState(x, base2cam);

	// tf_last * P1_last = tf_current => tf_last.inv() * tf_current = P1_last
	tf::Transform last2current = this->cameraTransformFromState(lf.state, base2cam).inverse() * tf_current;

	// tf_current * P2 = tf_kf => tf_kf * tf_current.inv = P2
	tf::Transform P2_temp = tf_current.inverse() * this->cameraTransformFromState(this->frameBuffer.at(kf.frameBufferIndex).state, base2cam);
	cv::Matx34d P2 = tfTransform2RtMatrix(P2_temp); // this is the transform which converts points in the currentframe to points in the keyframe

	//go through each current feature and transform its depth from the last frame
	for(auto& e : cf.features)
	{
		VIOFeature2D& last_ft = (e.isMatched()) ? lf.features.at(e.getMatchedIndex()) : e;
		//last_ft = lf.features.at(e.getMatchedIndex()); // get the last feature which matches this one
		if(!e.isMatched())
			continue;

		ROS_ASSERT(last_ft.getFeatureID() == e.getMatchedID());

		tf::Vector3 transformedPoint = last2current * (last_ft.getFeatureDepth() * tf::Vector3(last_ft.getUndistorted().x, last_ft.getUndistorted().y, 1.0)); // transform the 3d point from the last frame into the current frame

		//extract the depth from the transformed point and set it
		e.setFeatureDepth(transformedPoint.z()); // the depth would be equal to the new z from the transformed point
		//ROS_DEBUG_STREAM(e.getFeatureDepth());
	}

	cv::Matx<double, 6, 4> A;
	cv::vconcat(P1, P2, A); // construct the A in Ax = b

	//TODO - update the depths of all matched points
	//now that the depths are all corrected for the motion of the camera we can triangulate and do a kalman update on the feature
	Eigen::Matrix3d fundamental_matrix;
	Matrix3x4d pose1(P1.val);
	Matrix3x4d pose2(P2.val);

	Eigen::Vector4d X;

	double length_of_motion = P2_temp.getOrigin().length();

	this->FundamentalMatrixFromProjectionMatrices(P1.val, P2.val, fundamental_matrix.data());

	if(kf.pixelDelta > MIN_FUNDAMENTAL_PXL_DELTA)
	{

		for(int i = 0; i < kf.currentFrameIndexes.size(); i++)
		{
			cv::Point2f pt2=kf.matchedFeatures.at(i).getUndistorted(), pt1=cf.features.at(kf.currentFrameIndexes.at(i)).getUndistorted(); // get the two points
			Eigen::Vector2d u1, u2;
			u1(0) = pt1.x;
			u1(1) = pt1.y;
			u2(0) = pt2.x;
			u2(1) = pt2.y;

			this->Triangulate(pose1, pose2, u1, u2, &X, fundamental_matrix);

			double depth = X(2)/X(3);

			if(depth > MIN_TRIAG_Z)
			{
				//cf.features.at(kf.currentFrameIndexes.at(i)).updateDepth(depth, variance + 1 / (length_of_motion * depth + 0.00001)); // update the depth of the point

				ROS_DEBUG_STREAM("depth: " << depth << " variance: " << variance + 1 / (length_of_motion * depth + 0.00001));
				ROS_DEBUG_STREAM("reproj error ^: " << this->ReprojectionError(pose2, X, u2));
			}

		}
	}
}


void VIO::update3DFeatures()
{
>>>>>>> 98dd91d74806469f2705a7968f81ae787fbc7470
	cv::Mat F;
	cv::Matx33f R;
	cv::Matx31f t;
	std::vector<cv::Point2f> pt1, pt2;
	std::vector<VIOFeature2D> ft1, ft2;
	bool pass;
	double error;
	int match_frame_index;

	//TODO update 3d features using all keyframes
	//error = this->computeFundamentalMatrix(F, R, t, pt1, pt2, pass, ft1, ft2, match_frame_index);

	if (pass && error < MAXIMUM_FUNDAMENTAL_ERROR) {

		cv::Matx34f P1, P2;
		//cv::Mat X_;
		cv::Matx<float, 6, 4> A;

		cv::hconcat(cv::Mat::eye(cv::Size(3, 3), CV_32F),
				cv::Mat::zeros(cv::Size(1, 3), CV_32F), P1);
		cv::hconcat(R, t, P2);
		cv::vconcat(P1, P2, A);

		//cv::triangulatePoints(P1, P2, pt1, pt2, X_);
		//cv::Mat_<float> X = cv::Mat_<float>(4, pt1.size());
		//X_.copyTo(X);
		//ROS_ASSERT(ft2.size() == pt2.size() && ft2.size() == X.cols);

		tf::StampedTransform base2cam;
		try {
			this->ekf.tf_listener.lookupTransform(this->camera_frame,
					this->CoM_frame, ros::Time(0), base2cam);
		} catch (tf::TransformException& e) {
			ROS_WARN_STREAM(e.what());
		}

		std::vector<VIOFeature3D> inactives, actives;

		inactives = this->active3DFeatures;

		for (int i = 0; i < ft2.size(); i++) {
			VIOFeature3D matched3dFeature;
			bool matched3d = false;

			for (int j = 0; j < inactives.size(); j++) {
				if (inactives.at(j).current2DFeatureMatchIndex
						== ft2.at(i).getMatchedIndex()) {
					ROS_ASSERT(
							inactives.at(j).current2DFeatureMatchID
							== ft2.at(i).getMatchedID());
					matched3d = true;
					matched3dFeature = inactives.at(j);

					inactives.erase(inactives.begin() + j);

					break;
				}
			}

			cv::Matx41f X;
			cv::Matx61f b;

			b(0) = pt1.at(i).x;
			b(1) = pt1.at(i).y;
			b(2) = 1.0;
			b(3) = pt2.at(i).x;
			b(4) = pt2.at(i).y;
			b(5) = 1.0;

			cv::solve(A, b, X, cv::DECOMP_SVD);

			cv::Matx61f b_ = A * X;
			b_(0) /= b_(2);
			b_(1) /= b_(2);
			b_(2) = 1.0;
			b_(3) /= b_(5);
			b_(4) /= b_(5);
			b_(5) = 1.0;

			double reprojError = cv::norm(b_ - b);

			//CONVERT THE POINT INTO THE WORLD COORDINATE FRAME
			VIOState centerState = this->frameBuffer.at(match_frame_index).state;

			tf::Vector3 r_c = tf::Vector3(X(0) / X(3), X(1) / X(3),
					X(2) / X(3));

			tf::Vector3 r_b = tf::Vector3(centerState.x(), centerState.y(),
					centerState.z());
			tf::Quaternion q_b = tf::Quaternion(centerState.q1(),
					centerState.q2(), centerState.q3(), centerState.q0());
			tf::Transform w2b = tf::Transform(q_b, r_b);

			tf::Transform w2c = w2b * base2cam;

			tf::Vector3 r_w = w2c.inverse() * r_c;

			//ROS_DEBUG_STREAM("point: " << r_c.x() << ", " << r_c.y() << ", " << r_c.z());
			//ROS_DEBUG_STREAM("reproj error: " << reprojError);

			//UPDATE THE 3d POINT OR ADD IT

			if (r_c.z() > MIN_TRIAG_Z && reprojError < MAX_TRIAG_ERROR) {
				if (matched3d) {
					//ROS_DEBUG_STREAM("updating 3d feature");
					matched3dFeature.current2DFeatureMatchID =
							ft2.at(i).getFeatureID();
					matched3dFeature.current2DFeatureMatchIndex = i;
					matched3dFeature.update(
							Eigen::Vector3d(r_w.x(), r_w.y(), r_w.z()),
							reprojError);
					actives.push_back(matched3dFeature);
				} else {
					//ROS_DEBUG_STREAM("adding new 3d feature");
					VIOFeature3D ft3d;
					ft3d.current2DFeatureMatchID = ft2.at(i).getFeatureID();
					ft3d.current2DFeatureMatchIndex = i;
					ft3d.position = Eigen::Vector3d(r_w.x(), r_w.y(), r_w.z());
					ft3d.variance = reprojError;
					actives.push_back(ft3d);
				}
			} else {
				if (matched3d) {
					//ROS_DEBUG_STREAM("bad triag, preserving 3d point without update");
					matched3dFeature.current2DFeatureMatchID =
							ft2.at(i).getFeatureID();
					matched3dFeature.current2DFeatureMatchIndex = i;
					actives.push_back(matched3dFeature);
				}
			}

			if (matched3d) {
				tf::Vector3 pos = w2c
 * tf::Vector3(matched3dFeature.position(0),
								matched3dFeature.position(1),
								matched3dFeature.position(2));
				cv::Matx41f X_;
				X_(0) = pos.x();
				X_(1) = pos.y();
				X_(2) = pos.z();
				X_(3) = 1.0;
				cv::Matx31f b2 = P1 * X_;

				//ROS_DEBUG_STREAM("Projected 3d Point Error: " << tf::Vector3(b2(0)/b2(2) - b(0), b2(1)/b2(2) - b(1), 0).length());
			}

		}
		this->active3DFeatures = actives;
		this->inactive3DFeatures = inactives;
	} else {
		std::vector<VIOFeature3D> inactives, actives;

		inactives = this->active3DFeatures;

		for (int i = 0; i < ft2.size(); i++) {
			VIOFeature3D matched3dFeature;
			bool matched3d = false;

			for (int j = 0; j < inactives.size(); j++) {
				if (inactives.at(j).current2DFeatureMatchIndex
						== ft2.at(i).getMatchedIndex()) {
					ROS_ASSERT(
							inactives.at(j).current2DFeatureMatchID
							== ft2.at(i).getMatchedID());
					matched3d = true;
					matched3dFeature = inactives.at(j);

					inactives.erase(inactives.begin() + j);

					break;
				}
			}

			if (matched3d) {
				//ROS_DEBUG_STREAM("preserving 3d point without update");
				matched3dFeature.current2DFeatureMatchIndex = i;
				matched3dFeature.current2DFeatureMatchID =
						ft2.at(i).getFeatureID();
				actives.push_back(matched3dFeature);
			}
		}

		this->active3DFeatures = actives;
		this->inactive3DFeatures = inactives;
	}
}

 */
 